{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:33:43.705810Z",
     "start_time": "2024-06-30T16:33:43.692805Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import chess\n",
    "import chess.svg\n",
    "import chess.engine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.Game_playing import index_map\n",
    "from utils.Dataloading import fen_to_board"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_stockfish_data(N=2_000_000):\n",
    "\n",
    "    \"\"\"returns the boards, metadata, human moves and stockfish moves for N games\"\"\"\n",
    "\n",
    "    file_path = \"Data/StockData.csv\"\n",
    "    new_data = True\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    df = pd.read_csv(file_path, nrows=N)\n",
    "    ind_map = index_map.cpu().numpy()\n",
    "\n",
    "\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    fens = np.array(df['FENs'])\n",
    "    moves = np.zeros((len(fens), 128))\n",
    "    h_moves = np.zeros((len(fens), 128))\n",
    "\n",
    "    bitboards, meta = zip(*[fen_to_board(x) for x in tqdm(fens, total=len(fens))])\n",
    "\n",
    "    bitboards = np.asarray(bitboards)\n",
    "\n",
    "    meta = np.asarray(meta)\n",
    "\n",
    "    if new_data:\n",
    "\n",
    "        moves[df.index,df['stock_moves'].apply(lambda x: chess.Move.from_uci(x).from_square)] = 1\n",
    "        moves[df.index, 64 + df['stock_moves'].apply(lambda x: chess.Move.from_uci(x).to_square)] = 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i in tqdm(range(4)):\n",
    "\n",
    "            moves[df.index,df[f'move{i}'].apply(lambda x: chess.Move.from_uci(x).from_square)] += df[f'eval{i}'] + epsilon\n",
    "            moves[df.index, 64 + df[f'move{i}'].apply(lambda x: chess.Move.from_uci(x).to_square)] += df[f'eval{i}'] + epsilon\n",
    "\n",
    "\n",
    "    h_moves[df.index, df[\"hmoves\"].apply(lambda x: chess.Move.from_uci(x).from_square)] = 1\n",
    "    h_moves[df.index, 64 + df[\"hmoves\"].apply(lambda x: chess.Move.from_uci(x).to_square)] = 1\n",
    "\n",
    "    flipped_moves = np.zeros_like(moves)\n",
    "    flipped_moves[:,:64] = moves[:,ind_map]\n",
    "    flipped_moves[:,64:] = moves[:,64+ind_map]\n",
    "    #v_fens = np.vectorize(fen_to_board)\n",
    "    moves = np.where(np.expand_dims(meta, 1), moves, flipped_moves)\n",
    "\n",
    "    flipped_moves = np.zeros_like(h_moves)\n",
    "    flipped_moves[:,:64] = h_moves[:,ind_map]\n",
    "    flipped_moves[:,64:] = h_moves[:,64+ind_map]\n",
    "    #v_fens = np.vectorize(fen_to_board)\n",
    "    h_moves = np.where(np.expand_dims(meta, 1), h_moves, flipped_moves)\n",
    "\n",
    "    #bitboards = np.array(list(map(fen_to_board, fens)))\n",
    "\n",
    "    del df\n",
    "\n",
    "    return bitboards, meta, h_moves, moves"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:28:27.442765Z",
     "start_time": "2024-06-30T16:28:27.424766Z"
    }
   },
   "id": "2e6b88e00897e9ec",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [02:07<00:00, 3909.69it/s]\n"
     ]
    }
   ],
   "source": [
    "bitboards, meta, human_moves, ai_moves = load_stockfish_data(500_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:43:48.708777Z",
     "start_time": "2024-06-30T17:41:30.963842Z"
    }
   },
   "id": "ec30a4292282774c",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model architecture taken from behavioral_cloning.ipynb\n",
    "class AntiCheatBC(nn.Module):\n",
    "\n",
    "    def __init__(self, depth):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, 3, 1, padding=1, padding_mode = 'zeros')\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.depth = depth\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            self.layers.append(nn.Conv2d(64,64, 3, 1, padding=1, padding_mode = 'zeros'))\n",
    "            self.layers.append(nn.BatchNorm2d(64))\n",
    "            self.layers.append(nn.Conv2d(64,64, 3, 1, padding=1, padding_mode = 'zeros'))\n",
    "            self.layers.append(nn.BatchNorm2d(64))\n",
    "\n",
    "        self.linear = nn.Linear(4096, 1)\n",
    "        # self.out_linear = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = i*4\n",
    "            ph = x.clone()\n",
    "            ph = self.layers[j](ph)\n",
    "            ph = self.layers[j+1](ph)\n",
    "            ph = nn.functional.relu(ph)\n",
    "            ph = self.layers[j+2](ph)\n",
    "            ph = self.layers[j+3](ph)\n",
    "\n",
    "            x = x + ph\n",
    "            x = nn.functional.relu(x)\n",
    "\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # x = self.out_linear(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:47:25.445636Z",
     "start_time": "2024-06-30T17:47:25.431635Z"
    }
   },
   "id": "b3fe8a11cdb94091",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:53:13.742221Z",
     "start_time": "2024-06-30T16:53:13.732008Z"
    }
   },
   "id": "f66d3a2edef30553",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ChessDataConv(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, bitboards, white_turn, moves, ai_moves):\n",
    "\n",
    "        self.bitboards = torch.tensor(bitboards, dtype = torch.float).to(device)\n",
    "\n",
    "        self.moves = torch.tensor(moves, dtype = torch.float, device = device).view(-1,2,8,8)\n",
    "\n",
    "        self.ai_moves = torch.tensor(ai_moves, dtype = torch.float, device = device).view(-1,2,8,8)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.moves.size(dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            \n",
    "            return self.bitboards[idx], self.moves[idx], 0.0\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return self.bitboards[idx], self.ai_moves[idx], 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:39:30.989599Z",
     "start_time": "2024-06-30T17:39:30.974599Z"
    }
   },
   "id": "830652de33781999",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model architecture taken from behavioral_cloning.ipynb\n",
    "class MLPv2_1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(14, 64, 3, 1, padding=1, padding_mode = 'zeros')\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.depth = 6\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            self.layers.append(nn.Conv2d(64,64, 3, 1, padding=1, padding_mode = 'zeros'))\n",
    "            self.layers.append(nn.BatchNorm2d(64))\n",
    "            self.layers.append(nn.Conv2d(64,64, 3, 1, padding=1, padding_mode = 'zeros'))\n",
    "            self.layers.append(nn.BatchNorm2d(64))\n",
    "\n",
    "        self.linear = nn.Linear(4096, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = i*4\n",
    "            ph = x.clone()\n",
    "            ph = self.layers[j](ph)\n",
    "            ph = self.layers[j+1](ph)\n",
    "            ph = nn.functional.relu(ph)\n",
    "            ph = self.layers[j+2](ph)\n",
    "            ph = self.layers[j+3](ph)\n",
    "\n",
    "            x = x + ph\n",
    "            x = nn.functional.relu(x)\n",
    "\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        minn, ila = x[:,:64], x[:,64:]\n",
    "\n",
    "        return minn, ila\n",
    "\n",
    "#import the behavioral cloning model from saved models\n",
    "RDv2 = torch.load(\"Models/RDv2.3 CB.pt\", map_location= device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:38:34.817393Z",
     "start_time": "2024-06-30T17:38:34.770992Z"
    }
   },
   "id": "b30a59fdedff4c97",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = ChessDataConv(bitboards, meta, human_moves, ai_moves)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:43:49.616448Z",
     "start_time": "2024-06-30T17:43:48.711778Z"
    }
   },
   "id": "ab6c505e526dc9e6",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Mk1 = AntiCheatBC(2)\n",
    "\n",
    "optim = torch.optim.Adam(Mk1.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:47:28.678456Z",
     "start_time": "2024-06-30T17:47:28.663452Z"
    }
   },
   "id": "f694c717cd56ce01",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size = 64, shuffle = True, generator=torch.Generator(device=device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:43:55.385543Z",
     "start_time": "2024-06-30T17:43:55.374215Z"
    }
   },
   "id": "9ffc03c60700fce0",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7813 [00:00<?, ?it/s]C:\\Users\\osama\\AppData\\Local\\Temp\\ipykernel_3132\\3052395820.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  our_move = torch.cat((torch.tensor(our_move[0]).view(-1,1,8,8), torch.tensor(our_move[1]).view(-1,1,8,8)), dim=1)\n",
      "100%|██████████| 7813/7813 [00:54<00:00, 142.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1 with loss 0.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7813/7813 [00:55<00:00, 141.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2 with loss 0.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7813/7813 [00:55<00:00, 140.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3 with loss 0.6893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 271/7813 [00:01<00:54, 137.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[96], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m bitboards, moves, target \u001B[38;5;129;01min\u001B[39;00m tqdm(loader):\n\u001B[0;32m     13\u001B[0m     optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 14\u001B[0m     our_move \u001B[38;5;241m=\u001B[39m \u001B[43mRDv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbitboards\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     our_move \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((torch\u001B[38;5;241m.\u001B[39mtensor(our_move[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m8\u001B[39m), torch\u001B[38;5;241m.\u001B[39mtensor(our_move[\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m8\u001B[39m)), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     17\u001B[0m     output \u001B[38;5;241m=\u001B[39m Mk1(torch\u001B[38;5;241m.\u001B[39mcat((our_move, moves), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[69], line 40\u001B[0m, in \u001B[0;36mMLPv2_1.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     35\u001B[0m     x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[0;32m     38\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m minn, ila \u001B[38;5;241m=\u001B[39m x[:,:\u001B[38;5;241m64\u001B[39m], x[:,\u001B[38;5;241m64\u001B[39m:]\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m minn, ila\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Initial Training Loop\n",
    "eps = 20\n",
    "losses = []\n",
    "\n",
    "G = len(loader)\n",
    "\n",
    "for epoch in range(eps):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for bitboards, moves, target in tqdm(loader):\n",
    "\n",
    "        optim.zero_grad()\n",
    "        our_move = RDv2(bitboards)\n",
    "        our_move = torch.cat((torch.tensor(our_move[0]).view(-1,1,8,8), torch.tensor(our_move[1]).view(-1,1,8,8)), dim=1)\n",
    "\n",
    "        output = Mk1(torch.cat((our_move, moves), dim=1))\n",
    "\n",
    "        loss = criterion(output.view(-1), target.float())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss /= G\n",
    "\n",
    "    losses.append(running_loss)\n",
    "    print(f'Finished epoch {epoch+1} with loss {running_loss:.4f}')\n",
    "    if epoch % 5 == 4:\n",
    "        torch.save(Mk1,f\"ACbc_Mk1' ep{epoch}.pt\")\n",
    "\n",
    "    if epoch > 3 and losses[-1] > losses[-2]:\n",
    "        break\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:54:06.646156Z",
     "start_time": "2024-06-30T17:51:19.208848Z"
    }
   },
   "id": "61133aaac62bde0e",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 16, 8, 8])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitboards.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:41:24.085797Z",
     "start_time": "2024-06-30T17:41:24.078797Z"
    }
   },
   "id": "f5d79d4339e15692",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7813/7813 [00:31<00:00, 251.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5003679990768433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "for datum, label in tqdm(loader):\n",
    "\n",
    "    output = Mk1(datum)\n",
    "    output = nn.functional.sigmoid(output)\n",
    "\n",
    "    corr += torch.sum(label == torch.round(output).view(-1))\n",
    "    \n",
    "print(f\"Accuracy: {corr / len(loader) / 64}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:29:04.318562Z",
     "start_time": "2024-06-30T17:28:33.268635Z"
    }
   },
   "id": "6a0b118560db5ae2",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "500032"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader) * 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:25:56.490809Z",
     "start_time": "2024-06-30T17:25:56.483806Z"
    }
   },
   "id": "cc694e839cce79a0",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(250119.0625)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr / 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:21:16.743034Z",
     "start_time": "2024-06-30T17:21:16.724527Z"
    }
   },
   "id": "6360079d5c4b82b3",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed55885c78fdd0d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
